#!/bin/bash
########## Define Resources Needed with SBATCH Lines ##########
#SBATCH --time=04:00:00             # limit of wall clock time - how long the job will run (same as -t)
#SBATCH --nodes=4                   # Number of nodes
#SBATCH --ntasks-per-node=16        # Number of MPI tasks per node
#SBATCH --cpus-per-task=1           # number of CPUs (or cores) per task (same as -c)
#SBATCH --mem=8G                    # memory required per node - amount of memory (in bytes)
#SBATCH --job-name multiple_nodes   # you can give your job a name for easier identification (same as -J)
 
 
########## Command Lines to Run ##########

module purge
module load GCC/9.3.0 
module load intel/2021a

cd $SLURM_SUBMIT_DIR                    ### change to the directory where your code is located
 
# srun -n 4 a.out                       ### call your executable. srun kinda like mpiexec.
chmod +x run_pi_calc.sh
./run_pi_calc.sh
 
scontrol show job $SLURM_JOB_ID         ### write job information to output file
